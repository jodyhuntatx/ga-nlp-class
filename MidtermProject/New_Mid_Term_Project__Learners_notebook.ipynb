{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><p float=\"center\">\n",
        "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/>\n",
        "</p></center>\n",
        "\n",
        "<h1><center><font size=10> Generative AI for NLP Program</center></font></h1>\n",
        "<h1><center> Project </center></h1>"
      ],
      "metadata": {
        "id": "7A5on_E9xQSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GA-NLP Mid-Term Project: Financial Product Complaint Classification and Summarization**"
      ],
      "metadata": {
        "id": "pkypp72Oa4LD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Business Context**\n",
        "\n",
        "In today's financial landscape, customer complaints are pivotal for financial institutions, highlighting areas of dissatisfaction and guiding business improvements. The intricate task of classifying these complaints into specific product categories is crucial for understanding customer issues and enhancing service delivery. By employing Generative AI for text classification, businesses can gain a detailed understanding of customer grievances related to various financial products such as credit reports, student loans, and money transfers.\n",
        "\n",
        "The integration of machine learning algorithms has revolutionized the automation of customer complaint classification. Utilizing these advanced techniques, financial institutions can swiftly and accurately categorize new complaints based on their content. This automation not only saves time and resources but also ensures timely responses to customer issues, thereby improving customer satisfaction and compliance with regulatory standards.\n",
        "\n",
        "Additionally, this project will explore the summarization of customer narratives to provide more personalized solutions to complaints. By using Generative AI, businesses can enhance their ability to classify complaints more precisely and generate complaint summaries that facilitate more tailored and effective service responses.\n",
        "\n",
        "Embarking on this project of Financial Product Complaint Classification and Summarization, with a focus on classification and summarization accuracy, equips you with essential skills applicable to real-world business contexts. Through hands-on experience with code and implementation specifics, you'll gain the proficiency to build such solutions using open-source machine learning algorithms. This experience will serve as a compelling Proof-of-Concept, paving the way for the implementation of these advanced solutions in financial institutions."
      ],
      "metadata": {
        "id": "exBRHi0DbHFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Project Objective**\n",
        "\n",
        "Develop a Generative AI application using a Large Language Model to automate product classification and narrative summarization. This application will predict product categories, generate responses based on customer sentiment, and summarize narratives for the mediation team. We have been tackling this task with BERT and prompt engineering with LLMs. We will explore various techniques and select the most effective method."
      ],
      "metadata": {
        "id": "hJfHP45hZpwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Section 1 BERT Fine Tuning (10 Marks)**"
      ],
      "metadata": {
        "id": "9iz9eQKRzGBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 1: Installling the necessary packages and importing libraries (1 Mark)**"
      ],
      "metadata": {
        "id": "tg8yUa4sxWUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the libraries to load transformers models\n",
        "\"__________\""
      ],
      "metadata": {
        "id": "-9Z6hNnkxSMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import visualization libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import modules from scikit-learn for machine learning tasks\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
        "\n",
        "# Import TensorFlow for deep learning tasks\n",
        "import tensorflow as tf\n",
        "\n",
        "import re\n",
        "import json\n",
        "import"
      ],
      "metadata": {
        "id": "NwQFn8w-yloT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import BertTokenizer, TFBertForSequenceClassification from the Hugging Face transformers library\n",
        "\"__________\"\n"
      ],
      "metadata": {
        "id": "gl8ibWBGymnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seed for the TensorFlow random number generator to ensure reproducibility\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "FDdZSwFgTx9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 2: Data preprocessing for Bert Fine Tunning (2 Marks)**"
      ],
      "metadata": {
        "id": "mFyPpwimzm7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a CSV File containing Dataset of 500 products, narrative and summary (summary of narrative)\n",
        "data=\"__________\""
      ],
      "metadata": {
        "id": "d1iRBbNEz8AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Bert_data=data[['product','narrative']]"
      ],
      "metadata": {
        "id": "ZDJlPfhY0QtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dependent and independent variables from Bert_data\n",
        "train_test= \"__________\"\n",
        "y = \"__________\"\n",
        "# Further split the temporary set into train (80%) and test (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_test, y, test_size=0.20, stratify=y, random_state=42)"
      ],
      "metadata": {
        "id": "H0OWEvhk0l9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoding\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# fit the encoder to the training labels\n",
        "y_train_enc = \"__________\"\n",
        "\n",
        "# applying the encoder mapping from training labels to test labels\n",
        "y_test_enc = \"__________\""
      ],
      "metadata": {
        "id": "qscEhyvd1COV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 3: Tokenization (1 Mark)**"
      ],
      "metadata": {
        "id": "2z6YF8gZ1MTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading and creating an instance of the BERT tokenizer\n",
        "tokenizer = \"__________\"\n",
        "# specifying the maximum length of the input 512\n",
        "max_length = \"__________\""
      ],
      "metadata": {
        "id": "xg5RWZtp2Puu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tokenized = tokenizer(\n",
        "    X_train.values.tolist(),    # passing the data as a list to the tokenizer\n",
        "    max_length=max_length,    # specifies the maximum length of the tokenized data\n",
        "    padding='max_length',    # padding the data to the specified maximum length\n",
        "    truncation=True,    # truncating the input if it is longer than the specified maximum length\n",
        "    return_attention_mask=True,    # specifying to return attention masks\n",
        "    return_tensors='tf',    # specifying to return the output as tensorflow tensors\n",
        ")\n",
        "X_test_tokenized = tokenizer(\n",
        "    X_test.values.tolist(),\n",
        "    max_length=max_length,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='tf',\n",
        ")"
      ],
      "metadata": {
        "id": "vPb5FlFR2UAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 4: Creating Tensorflow dataset (1 Mark)**"
      ],
      "metadata": {
        "id": "raT2V3lU2x_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the size of the batches\n",
        "batch_size = 8\n",
        "\n",
        "# convert the tokenized input and the output into a batched tensorflow dataset for training\n",
        "train_tokenized_tf = \"__________\"\n",
        "\n",
        "\n",
        "# convert the tokenized input and the output into a batched tensorflow dataset for testing\n",
        "test_tokenized_tf = \"__________\""
      ],
      "metadata": {
        "id": "63e-GNZT2wkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 5 Evaluating the base model's performance in product classification.(1 Marks)**"
      ],
      "metadata": {
        "id": "rSSSVjqe3M54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_f1_score(actual_val,perdicted_val):\n",
        "    micro_f1_score = f1_score(actual_val, preds_test, average=\"micro\")\n",
        "    return micro_f1_score"
      ],
      "metadata": {
        "id": "i7mCVcuE4Dzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual product class\n",
        "actual_val = np.concatenate([y for x, y in test_tokenized_tf], axis=0)"
      ],
      "metadata": {
        "id": "dL5hdRna6MW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = y.nunique()\n",
        "# Initialize Model using BERT for sequence classification\n",
        "model = \"__________\""
      ],
      "metadata": {
        "id": "KnQnyS-93G9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make prediction on test_tokenized_tf\n",
        "preds_raw_test = \"__________\"\n",
        "preds_test = np.argmax(np.array(tf.nn.softmax(preds_raw_test.logits)), axis=1)"
      ],
      "metadata": {
        "id": "RpKM4Ebg4sO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate bert base model\n",
        "f1_score= \"__________\"\n",
        "print(f1_score)"
      ],
      "metadata": {
        "id": "q9oa8YPR5D0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 6 Fine-Tuning Bert Model on training set (2 Marks)**"
      ],
      "metadata": {
        "id": "4UVe2kiB510Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = y.nunique()\n",
        "# Model initialization using BERT for sequence classification\n",
        "model =  \"__________\""
      ],
      "metadata": {
        "id": "Shjp4bPVlfMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the learning rate for the optimizer\n",
        "learning_rate = 1e-5\n",
        "\n",
        "# Setting the optimizer to Adam\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
        "\n",
        "# Specify the loss function for the model\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Define evaluation metric(s) for the model\n",
        "metric = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
        "\n",
        "# Compile the model with the chosen optimizer, loss function, and metrics\n",
        "\"__________\""
      ],
      "metadata": {
        "id": "f1Psz_7a59TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate class weights for imbalanced dataset\n",
        "cw = (y_train_enc.shape[0]) / np.bincount(y_train_enc)\n",
        "\n",
        "# Create a dictionary mapping class indices to their respective class weights\n",
        "cw_dict = {}\n",
        "for i in range(cw.shape[0]):\n",
        "    cw_dict[encoder.transform(encoder.classes_)[i]] = cw[i]"
      ],
      "metadata": {
        "id": "ZJiQN-mb6kIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of training epochs\n",
        "n_epochs = 1\n",
        "#train bert model\n",
        "bert_base_tuned = \"__________\""
      ],
      "metadata": {
        "id": "YzNLc2n96oT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 7. Evaluating the trained model performance (1 Mark)**"
      ],
      "metadata": {
        "id": "NQ43M02s68Uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate raw predictions on the test dataset using the trained model\n",
        "preds_raw_val =  \"__________\"\n",
        "\n",
        "# Extract predicted labels by finding the index with the highest probability for each example\n",
        "preds_val = np.argmax(np.array(tf.nn.softmax(preds_raw_val.logits)), axis=1)"
      ],
      "metadata": {
        "id": "wd890cvZ61_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate bert trained model\n",
        "f1_score= \"__________\"\n",
        "print(f1_score)"
      ],
      "metadata": {
        "id": "D33p2Wqn7Tgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 8: Write your observations (1 Mark)**"
      ],
      "metadata": {
        "id": "8iBOLR6a7dbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prompt Engineering**"
      ],
      "metadata": {
        "id": "X_wRqgV3B_8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Section 2: Install Libraries for Prompt Engineering and Setting up Mistral Model (3 Marks)**"
      ],
      "metadata": {
        "id": "sSiuLz5JCLQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 9: Install neccessary libraries (1 Mark)**"
      ],
      "metadata": {
        "id": "XcGMygn3_Mq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation for GPU llama_cpp_python==0.2.28\n",
        " \"__________\"\n",
        "# For downloading the models from HF Hub huggingface-hub==0.23.2\n",
        " \"__________\"\n",
        "# install evaluate==0.4.2 and bert-score==0.3.13 using pip command\n",
        " \"__________\"\n",
        "# install numpy==1.25.2\n",
        " \"__________\""
      ],
      "metadata": {
        "id": "gl0E64ce8Uct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Imports for Libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import evaluate\n",
        "\n",
        "from google.colab import drive\n",
        "import locale"
      ],
      "metadata": {
        "id": "TsrUPUGTANTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 10: Importing Libaries and Setting up Mistral Model (2 Marks)**"
      ],
      "metadata": {
        "id": "GdQdNav7AOtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/blob/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf"
      ],
      "metadata": {
        "id": "NO7TfAEiSxMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Import Hf_hub_download from hugging_face_hub\n",
        "## Import Llama from llama_cpp\n",
        "\"__________\"\n",
        "\"__________\""
      ],
      "metadata": {
        "id": "cpQlIfeqAkvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model name or path as a string (You can find this info from hugging face website) Use Mistral\n",
        "\n",
        "model_name_or_path = \"__________\"\n",
        "\n",
        "# Define the model basename as a string, indicating it's in the gguf format\n",
        "\n",
        "model_basename = \"__________\" # the model is in gguf format"
      ],
      "metadata": {
        "id": "e9c3ISlwftKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(\n",
        "    repo_id=model_name_or_path,\n",
        "    filename=model_basename\n",
        "    )"
      ],
      "metadata": {
        "id": "vvPmPoy4wCIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the 'Llama' class with specified parameters\n",
        "# remove the blank spaces and complete the code\n",
        "\n",
        "lcpp_llm = Llama(\n",
        "        model_path=_____,\n",
        "        n_threads=___,  # CPU cores\n",
        "        n_batch=___,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "        n_gpu_layers=___,  # Change this value based on your model and your GPU VRAM pool.\n",
        "        n_ctx=____,  # Context window\n",
        "    )"
      ],
      "metadata": {
        "id": "R7Sm6O-jfzRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Section 3: Text to Label (12 Marks)**"
      ],
      "metadata": {
        "id": "Cj8Gk-m1Iz1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Zero-Shot Prompting (6 Marks)**"
      ],
      "metadata": {
        "id": "f6xcR3o2ga6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q11: Define the Prompt Template, System Message, generate_prompt** **(3 Marks)**\n",
        "\n",
        "Define a **system message** as a string and assign it to the variable system_message to generate product class.\n",
        "\n",
        "Create a **zero shot prompt template** that incorporates the system message and user input.\n",
        "\n",
        "Define **generate_prompt** function that takes both the system_message and user_input as arguments and formats them into a prompt template\n",
        "\n",
        "\n",
        "Write a Python function called **generate_mistral_response** that takes a single parameter, narrative, which represents the user's complain. Inside the function, you should perform the following tasks:\n",
        "\n",
        "\n",
        "- **Combine the system_message and narrative to create a prompt string using generate_prompt function.**\n",
        "\n",
        "*Generate a response from the Mistral model using the lcpp_llm instance with the following parameters:*\n",
        "\n",
        "- prompt should be the combined prompt string.\n",
        "- max_tokens should be set to 1200.\n",
        "- temperature should be set to 0.\n",
        "- top_p should be set to 0.95.\n",
        "- repeat_penalty should be set to 1.2.\n",
        "- top_k should be set to 50.\n",
        "- stop should be set as a list containing '/s'.\n",
        "- echo should be set to False.\n",
        "Extract and return the response text from the generated response.\n",
        "\n",
        "Don't forget to provide a value for the system_message variable before using it in the function."
      ],
      "metadata": {
        "id": "PZ8ual1jghmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"__________\""
      ],
      "metadata": {
        "id": "QIMDLp51ghGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_prompt_template = \"__________\""
      ],
      "metadata": {
        "id": "e24qaY7ug2CC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function that combines user_prompt and system_message to create the prompt\n",
        "def generate_prompt(system_message,user_input):\n",
        "    prompt = \"__________\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "JKOyehERgy77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_mistral_response(input_text):\n",
        "\n",
        "    # Combine user_prompt and system_message to create the prompt\n",
        "    prompt = \"__________\"\n",
        "\n",
        "    # Generate a response from the LLaMA model\n",
        "    response = lcpp_llm(\n",
        "    )\n",
        "\n",
        "    # Extract and return the response text\n",
        "    response_text = response[\"choices\"][0][______]  ### Fill in the blank\n",
        "    return response_text"
      ],
      "metadata": {
        "id": "3PJcu3dwg7FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Due to limited GPU resources, we will test our model with zero prompts on only 50 examples instead of the entire dataset.**"
      ],
      "metadata": {
        "id": "CmFU17EpyeKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly select 50 rows\n",
        "new_data = data.sample(n=50, random_state=40)"
      ],
      "metadata": {
        "id": "xhwItJrIhNta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q12: Create a new column in the DataFrame called 'mistral_response' and populate it with responses generated by applying the 'generate_mistral_response' function to each 'narrative' in the DataFrame and prepare the mistral_response_cleaned column using extract_category function** **(1 Marks)**"
      ],
      "metadata": {
        "id": "n-GrdMrVkaei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example - new_data['mistral_response'] = new_data['narrative'].apply(lambda x:______ )\n",
        "new_data['mistral_response'] = \"______ \""
      ],
      "metadata": {
        "id": "BqEme1elh24Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data['mistral_response'] = \"______ \""
      ],
      "metadata": {
        "id": "K1CBbpD3m_rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_category(text):\n",
        "    # Define the regex pattern to match \"category:\" or \"Category:\" followed by a word\n",
        "    pattern = r'category:\\s*(\\w+)'  # The pattern itself remains the same\n",
        "\n",
        "    # Use re.search with the re.IGNORECASE flag to make it case-insensitive\n",
        "    match = re.search(pattern, text, re.IGNORECASE)\n",
        "\n",
        "    # If a match is found, return the captured group, else return None\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        pattern1 = r'(credit_card|retail_banking|credit_reporting|mortgages_and_loans|debt_collection)'\n",
        "        match = re.search(pattern1, text, re.IGNORECASE)\n",
        "        if match:\n",
        "            return match.group()\n",
        "        else:\n",
        "            return ''"
      ],
      "metadata": {
        "id": "10Bulypzkwjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example - new_data['mistral_response_cleaned'] = new_data['narrative'].apply(lambda x:______ )\n",
        "new_data['mistral_response_cleaned'] = \"______ \""
      ],
      "metadata": {
        "id": "w-REbjsCnhaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q14: Calculate the F1 score** **(1 Marks)**"
      ],
      "metadata": {
        "id": "T9t6gsZVsL2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate F1 score for 'product' and 'mistral_response'\n",
        "f1 =  \"______ \"\n",
        "\n",
        "print(f'F1 Score: {f1}')"
      ],
      "metadata": {
        "id": "CeUoZb4ttDCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate F1 score for 'product' and 'mistral_response_cleaned'\n",
        "f2 =  \"______ \"\n",
        "print(f'F1 Score: {f2}')"
      ],
      "metadata": {
        "id": "ynzQvTi8tCcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q15: Explain the difference in F1 scores between mistral_response and mistral_response_cleaned.** **(1 Marks)**"
      ],
      "metadata": {
        "id": "803ZuG7OtSSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Few-Shot Prompting (6 Marks)**"
      ],
      "metadata": {
        "id": "T0YUCSXAnx3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q16: Prepare examples for a few-shot prompt, formulate the prompt, and generate the Mistral response. (4 Marks)**"
      ],
      "metadata": {
        "id": "wgxdRjW_wHId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate a set of gold examples by randomly selecting 10 instances of user_input and assistant_output from dataset ensuring a balanced representation with 2 examples from each class.**"
      ],
      "metadata": {
        "id": "hPuH7_ravBtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate positive and negative reviews\n",
        "import json\n",
        "review_1 = data[data['product'] == 'credit_card']\n",
        "review_2 = data[data['product'] == 'retail_banking']\n",
        "review_3 = data[data['product'] == 'credit_reporting']\n",
        "review_4 = data[data['product'] == 'mortgages_and_loans']\n",
        "review_5 = data[data['product'] == 'debt_collection']\n",
        "\n",
        "# Sample 3 positive and 3 negative reviews for gold examples\n",
        "gold_examples_1 = review_1.sample(2, random_state=40)\n",
        "gold_examples_2 = review_2.sample(2, random_state=40)\n",
        "gold_examples_3 = review_3.sample(2, random_state=40)\n",
        "gold_examples_4 = review_4.sample(2, random_state=40)\n",
        "gold_examples_5 = review_5.sample(2, random_state=40)\n",
        "\n",
        "# Concatenate positive and negative gold examples\n",
        "gold_examples_df = pd.concat([gold_examples_1,gold_examples_2,gold_examples_3,gold_examples_4,gold_examples_5 ])\n",
        "\n",
        "# Create the training set by excluding gold examples\n",
        "test_df = data.drop(index=gold_examples_df.index)\n",
        "\n",
        "# Convert gold examples to JSON\n",
        "columns_to_select = ['narrative', 'product']\n",
        "gold_examples_json = gold_examples_df[columns_to_select].to_json(orient='records')\n",
        "\n",
        "# Print the first record from the JSON\n",
        "print(json.loads(gold_examples_json)[0])\n",
        "\n",
        "# Print the shapes of the datasets\n",
        "print(\"Test Set Shape:\", examples_df.shape)\n",
        "print(\"Gold Examples Shape:\", gold_examples_df.shape)"
      ],
      "metadata": {
        "id": "iO9Wj19_n_LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define your **system_message**.\n",
        "\n",
        "Define **first_turn_template**, **example_template** and **prediction template**\n",
        "\n",
        "**create few shot prompt** using gold examples and system_message\n",
        "\n",
        "Randomly select 50 rows from test_df as test_data\n",
        "\n",
        "Create **mistral_response** and **mistral_response_cleaned** columns"
      ],
      "metadata": {
        "id": "1i8taCkTwqOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"______ \""
      ],
      "metadata": {
        "id": "iPzpBeWmzSIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_turn_template = \"______ \"\n",
        "examples_template = \"______ \"\n",
        "prediction_template = \"______ \""
      ],
      "metadata": {
        "id": "2bPjQJs_zRoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_few_shot_prompt(system_message, examples):\n",
        "\n",
        "    \"\"\"\n",
        "    Return a prompt message in the format expected by Mistral 7b.\n",
        "    10 examples are selected randomly as golden examples to form the\n",
        "    few-shot prompt.\n",
        "    We then loop through each example and parse the narrative as the user message\n",
        "    and the product as the assistant message.\n",
        "\n",
        "    Args:\n",
        "        system_message (str): system message with instructions for classification\n",
        "        examples(DataFrame): A DataFrame with examples (product + narrative + summary)\n",
        "        to form the few-shot prompt.\n",
        "\n",
        "    Output:\n",
        "        few_shot_prompt (str): A prompt string in the Mistral format\n",
        "    \"\"\"\n",
        "\n",
        "    few_shot_prompt = ''\n",
        "\n",
        "    columns_to_select = \"__________\"\n",
        "\n",
        "    examples = (\n",
        "        examples_df.loc[:, columns_to_select].to_json(orient='records')\n",
        "    )\n",
        "\n",
        "    for idx, example in enumerate(json.loads(examples)):\n",
        "        user_input_example = \"__________\"\n",
        "        assistant_output_example = \"__________\"\n",
        "\n",
        "        if idx == 0:\n",
        "            few_shot_prompt += mistral_first_turn_template.format(\n",
        "                system_message=system_message,\n",
        "                user_message=user_input_example,\n",
        "                assistant_message=assistant_output_example\n",
        "            )\n",
        "        else:\n",
        "            few_shot_prompt += mistral_examples_template.format(\n",
        "                user_message=user_input_example,\n",
        "                assistant_message=assistant_output_example\n",
        "            )\n",
        "\n",
        "    return few_shot_prompt"
      ],
      "metadata": {
        "id": "X07_kNji0vTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = \"______ \""
      ],
      "metadata": {
        "id": "yHfPlD9o1G7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(few_shot_prompt,new_review):\n",
        "    prompt =  \"______ \"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "EmTmY2oq3PRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_mistral_response(input_text):\n",
        "\n",
        "    # Combine user_prompt and system_message to create the prompt\n",
        "    prompt = \"__________\"\n",
        "\n",
        "    # Generate a response from the LLaMA model\n",
        "    response = lcpp_llm(\n",
        "    )\n",
        "\n",
        "    # Extract and return the response text\n",
        "    response_text = response[\"choices\"][0][______]  ### Fill in the blank\n",
        "    return response_text"
      ],
      "metadata": {
        "id": "GKrndCFRCJhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly select 50 rows\n",
        "test_data = data.sample(n=50, random_state=40)"
      ],
      "metadata": {
        "id": "FKYOjm7LDLJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['mistral_response'] = \"______ \""
      ],
      "metadata": {
        "id": "JdsjoaP4DXIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['mistral_response_cleaned'] = \"______ \""
      ],
      "metadata": {
        "id": "rA9KYTgxDh2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Calculate F1 score (1 Mark)**"
      ],
      "metadata": {
        "id": "plQNsfl0EDSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate F1 score for 'product' and 'mistral_response_cleaned'\n",
        "f3 =  \"______ \"\n",
        "print(f'F1 Score: {f3}')"
      ],
      "metadata": {
        "id": "7WXEgmPKEIoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q17: Share your observations on the few-shot and zero-shot prompt techniques. (1 Marks)**"
      ],
      "metadata": {
        "id": "IdtvdEnTEZcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Section 4: Text to Text generation (5 Marks)**"
      ],
      "metadata": {
        "id": "Vca9B37WGneH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a **system message** as a string and assign it to the variable system_message to generate product class.** (1 Mark)**\n",
        "\n",
        "Create a **zero shot prompt template** that incorporates the system message and user input.\n",
        "\n",
        "Define **generate_prompt** function that takes both the system_message and user_input as arguments and formats them into a prompt template\n",
        "\n",
        "\n",
        "Write a Python function called **generate_mistral_response** that takes a single parameter, narrative, which represents the user's complain. Inside the function, you should perform the following tasks:\n",
        "\n",
        "\n",
        "- **Combine the system_message and narrative to create a prompt string using generate_prompt function.**\n",
        "\n",
        "*Generate a response from the Mistral model using the lcpp_llm instance with the following parameters:*\n",
        "\n",
        "- prompt should be the combined prompt string.\n",
        "- max_tokens should be set to 1200.\n",
        "- temperature should be set to 0.\n",
        "- top_p should be set to 0.95.\n",
        "- repeat_penalty should be set to 1.2.\n",
        "- top_k should be set to 50.\n",
        "- stop should be set as a list containing '/s'.\n",
        "- echo should be set to False.\n",
        "Extract and return the response text from the generated response.\n",
        "\n",
        "Don't forget to provide a value for the system_message variable before using it in the function."
      ],
      "metadata": {
        "id": "X0FEJZSDL2U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"__________\""
      ],
      "metadata": {
        "id": "7PewmIuBII56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_prompt_template = \"__________\""
      ],
      "metadata": {
        "id": "czkz3BMpL2ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function that combines user_prompt and system_message to create the prompt\n",
        "def generate_prompt(system_message,user_input):\n",
        "    prompt = \"__________\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "WlY1x6ACLtlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_mistral_response(input_text):\n",
        "\n",
        "    # Combine user_prompt and system_message to create the prompt\n",
        "    prompt = \"__________\"\n",
        "\n",
        "    # Generate a response from the LLaMA model\n",
        "    response = lcpp_llm(\n",
        "    )\n",
        "\n",
        "    # Extract and return the response text\n",
        "    response_text = response[\"choices\"][0][______]  ### Fill in the blank\n",
        "    return response_text"
      ],
      "metadata": {
        "id": "S-kqoM4YL1xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q19: Generate mistral_response column containing LLM generated summaries** **(1 Marks)**"
      ],
      "metadata": {
        "id": "CpiIHpThMtW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly select 50 rows\n",
        "test_data = data.sample(n=50, random_state=40)"
      ],
      "metadata": {
        "id": "Fqxl54MMMBTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['mistral_response'] = \"______ \""
      ],
      "metadata": {
        "id": "1kWS3MmUMEgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q20: Evaluate bert score** **(2 Marks)**"
      ],
      "metadata": {
        "id": "dl-5fRKbND2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_score(result, scorer, bert_score=False):\n",
        "\n",
        "    \"\"\"\n",
        "    Return the ROUGE score or BERTScore for predictions on gold examples\n",
        "    For each example we make a prediction using the prompt.\n",
        "    Gold summaries and the AI generated summaries are aggregated into lists.\n",
        "    These lists are used by the corresponding scorers to compute metrics.\n",
        "    Since BERTScore is computed for each candidate-reference pair, we take the\n",
        "    average F1 score across the gold examples.\n",
        "\n",
        "    Args:\n",
        "        prompt (List): list of messages in the Open AI prompt format\n",
        "        gold_examples (str): JSON string with list of gold examples\n",
        "        scorer (function): Scorer function used to compute the ROUGE score or the\n",
        "                           BERTScore\n",
        "        bert_score (boolean): A flag variable that indicates if BERTScore should\n",
        "                              be used as the metric.\n",
        "\n",
        "    Output:\n",
        "        score (float): BERTScore or ROUGE score computed by comparing model predictions\n",
        "                       with ground truth\n",
        "    \"\"\"\n",
        "\n",
        "    model_predictions = result['mistral_response'].tolist\n",
        "    ground_truths = result['summary'].tolist()\n",
        "    if bert_score:\n",
        "        score = scorer.compute(\n",
        "            predictions=model_predictions,\n",
        "            references=ground_truths,\n",
        "            lang=\"en\",\n",
        "            rescale_with_baseline=True\n",
        "        )\n",
        "\n",
        "        return sum(score['f1'])/len(score['f1'])\n",
        "    else:\n",
        "        return scorer.compute(\n",
        "            predictions=model_predictions,\n",
        "            references=ground_truths\n",
        "        )"
      ],
      "metadata": {
        "id": "H6SqvLIeMVN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_scorer = \"__________\""
      ],
      "metadata": {
        "id": "4T4k6rA_MlS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = \"__________\"\n",
        "print(f'BERTScore: {score}')"
      ],
      "metadata": {
        "id": "ikncTGPBNPRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q21: Write your observation** **(1 Marks)**"
      ],
      "metadata": {
        "id": "-p6_jPcVNWz7"
      }
    }
  ]
}