{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7A5on_E9xQSC"
   },
   "source": [
    "<center><p float=\"center\">\n",
    "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/>\n",
    "</p></center>\n",
    "\n",
    "<h1><center><font size=10> Generative AI for NLP Program</center></font></h1>\n",
    "<h1><center> Project </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkypp72Oa4LD"
   },
   "source": [
    "# **GA-NLP Mid-Term Project: Financial Product Complaint Classification and Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exBRHi0DbHFT"
   },
   "source": [
    "## **Business Context**\n",
    "\n",
    "In today's financial landscape, customer complaints are pivotal for financial institutions, highlighting areas of dissatisfaction and guiding business improvements. The intricate task of classifying these complaints into specific product categories is crucial for understanding customer issues and enhancing service delivery. By employing Generative AI for text classification, businesses can gain a detailed understanding of customer grievances related to various financial products such as credit reports, student loans, and money transfers.\n",
    "\n",
    "The integration of machine learning algorithms has revolutionized the automation of customer complaint classification. Utilizing these advanced techniques, financial institutions can swiftly and accurately categorize new complaints based on their content. This automation not only saves time and resources but also ensures timely responses to customer issues, thereby improving customer satisfaction and compliance with regulatory standards.\n",
    "\n",
    "Additionally, this project will explore the summarization of customer narratives to provide more personalized solutions to complaints. By using Generative AI, businesses can enhance their ability to classify complaints more precisely and generate complaint summaries that facilitate more tailored and effective service responses.\n",
    "\n",
    "Embarking on this project of Financial Product Complaint Classification and Summarization, with a focus on classification and summarization accuracy, equips you with essential skills applicable to real-world business contexts. Through hands-on experience with code and implementation specifics, you'll gain the proficiency to build such solutions using open-source machine learning algorithms. This experience will serve as a compelling Proof-of-Concept, paving the way for the implementation of these advanced solutions in financial institutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJfHP45hZpwW"
   },
   "source": [
    "## **Project Objective**\n",
    "\n",
    "Develop a Generative AI application using a Large Language Model to automate product classification and narrative summarization. This application will predict product categories, generate responses based on customer sentiment, and summarize narratives for the mediation team. We have been tackling this task with BERT and prompt engineering with LLMs. We will explore various techniques and select the most effective method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iz9eQKRzGBr",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Section 1 BERT Fine Tuning (10 Marks)** (product classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tg8yUa4sxWUI"
   },
   "source": [
    "### **Question 1: Installing the necessary packages and importing libraries (1 Mark)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NwQFn8w-yloT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 13:51:22.567607: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-30 13:51:22.584338: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 13:51:22.604517: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 13:51:22.610647: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 13:51:22.625526: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 13:51:23.702181: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import modules from scikit-learn for machine learning tasks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
    "\n",
    "# Import TensorFlow for deep learning tasks\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gl8ibWBGymnB"
   },
   "outputs": [],
   "source": [
    "# Import BertTokenizer, TFBertForSequenceClassification from the Hugging Face transformers library\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FDdZSwFgTx9i"
   },
   "outputs": [],
   "source": [
    "# Set the seed for the TensorFlow random number generator to ensure reproducibility\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFyPpwimzm7C"
   },
   "source": [
    "### **Question 2: Data preprocessing for Bert Fine Tuning (2 Marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "d1iRBbNEz8AS"
   },
   "outputs": [],
   "source": [
    "# Load a CSV File containing Dataset of 500 products, narrative and summary (summary of narrative)\n",
    "data=pd.read_csv(\"./Complaints_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>narrative</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>credit_card</td>\n",
       "      <td>purchase order day shipping amount receive pro...</td>\n",
       "      <td>The customer made a purchase order with an agr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>credit_card</td>\n",
       "      <td>forwarded message date tue subject please inve...</td>\n",
       "      <td>The sender of the email believes they have bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>retail_banking</td>\n",
       "      <td>forwarded message cc sent friday pdt subject f...</td>\n",
       "      <td>The sender of the email alleges that Wells Far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>credit_reporting</td>\n",
       "      <td>payment history missing credit report speciali...</td>\n",
       "      <td>The credit report from Specialized Loan Servic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>credit_reporting</td>\n",
       "      <td>payment history missing credit report made mis...</td>\n",
       "      <td>The text concerns a person who found an unauth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            product                                          narrative  \\\n",
       "0       credit_card  purchase order day shipping amount receive pro...   \n",
       "1       credit_card  forwarded message date tue subject please inve...   \n",
       "2    retail_banking  forwarded message cc sent friday pdt subject f...   \n",
       "3  credit_reporting  payment history missing credit report speciali...   \n",
       "4  credit_reporting  payment history missing credit report made mis...   \n",
       "\n",
       "                                             summary  \n",
       "0  The customer made a purchase order with an agr...  \n",
       "1  The sender of the email believes they have bee...  \n",
       "2  The sender of the email alleges that Wells Far...  \n",
       "3  The credit report from Specialized Loan Servic...  \n",
       "4  The text concerns a person who found an unauth...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- product column is categorical\n",
    "- narrative text is already all lower case and needs no cleaning.\n",
    "- summary is mixed-case, may need cleaning later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZDJlPfhY0QtU"
   },
   "outputs": [],
   "source": [
    "#Bert_data=data['product','narrative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H0OWEvhk0l9M"
   },
   "outputs": [],
   "source": [
    "# Creating dependent and independent variables from Bert_data\n",
    "train_test = data['narrative']\n",
    "y = data['product']\n",
    "# Further split the temporary set into train (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_test, y, test_size=0.20, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400,), (400,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,), (100,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qscEhyvd1COV"
   },
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# fit the encoder to the training labels\n",
    "y_train_enc = encoder.fit_transform(y_train)\n",
    "\n",
    "# applying the encoder mapping from training labels to test labels\n",
    "y_test_enc = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2z6YF8gZ1MTc"
   },
   "source": [
    "### **Question 3: Tokenization (1 Mark)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xg5RWZtp2Puu"
   },
   "outputs": [],
   "source": [
    "# loading and creating an instance of the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "# specifying the maximum length of the input 512\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vPb5FlFR2UAk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722347511.568109   35076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722347511.572620   35076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722347511.575499   35076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722347511.578935   35076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722347511.581761   35076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722347511.584394   35076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722347511.752650   35076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722347511.753837   35076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722347511.754822   35076 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-30 13:51:51.755761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13775 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "X_train_tokenized = tokenizer(\n",
    "    X_train.values.tolist(),    # passing the data as a list to the tokenizer\n",
    "    max_length=max_length,    # specifies the maximum length of the tokenized data\n",
    "    padding='max_length',    # padding the data to the specified maximum length\n",
    "    truncation=True,    # truncating the input if it is longer than the specified maximum length\n",
    "    return_attention_mask=True,    # specifying to return attention masks\n",
    "    return_tensors='tf',    # specifying to return the output as tensorflow tensors\n",
    ")\n",
    "X_test_tokenized = tokenizer(\n",
    "    X_test.values.tolist(),\n",
    "    max_length=max_length,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='tf',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raT2V3lU2x_Q"
   },
   "source": [
    "### **Question 4: Creating Tensorflow dataset (1 Mark)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "63e-GNZT2wkP"
   },
   "outputs": [],
   "source": [
    "# defining the size of the batches\n",
    "batch_size = 8\n",
    "\n",
    "# convert the tokenized input and the output into a batched tensorflow dataset for training\n",
    "train_tokenized_tf = tf.data.Dataset.from_tensor_slices((dict(X_train_tokenized), y_train_enc)).batch(batch_size)\n",
    "\n",
    "# convert the tokenized input and the output into a batched tensorflow dataset for testing\n",
    "test_tokenized_tf = tf.data.Dataset.from_tensor_slices((dict(X_test_tokenized), y_test_enc)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSSSVjqe3M54"
   },
   "source": [
    "### **Question 5 Evaluating the base model's performance in product classification.(1 Marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "i7mCVcuE4Dzn"
   },
   "outputs": [],
   "source": [
    "def bert_f1_score(actual_vals, pred_vals):\n",
    "    micro_f1_score = f1_score(actual_vals, pred_vals, average=\"micro\")\n",
    "    return micro_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "dL5hdRna6MW_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 13:58:54.409215: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Actual product class\n",
    "actual_val = np.concatenate([y for x, y in test_tokenized_tf], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = y.nunique()\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "KnQnyS-93G9O"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model using BERT for sequence classification\n",
    "base_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3845      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109486085 (417.66 MB)\n",
      "Trainable params: 109486085 (417.66 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# print the summary of the model\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 10s 276ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make prediction on test_tokenized_tf\n",
    "preds_raw_test = base_model.predict(test_tokenized_tf)\n",
    "preds_test_base = np.argmax(np.array(tf.nn.softmax(preds_raw_test.logits)), axis=1)\n",
    "preds_test_base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test_base[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_enc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product\n",
       "credit_reporting       77\n",
       "mortgages_and_loans     7\n",
       "debt_collection         6\n",
       "credit_card             6\n",
       "retail_banking          4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product\n",
       "credit_reporting       388\n",
       "mortgages_and_loans     36\n",
       "debt_collection         29\n",
       "credit_card             28\n",
       "retail_banking          19\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "q9oa8YPR5D0z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09\n"
     ]
    }
   ],
   "source": [
    "# Evaluate bert base model\n",
    "base_f1_score = bert_f1_score(y_test_enc, preds_test_base)\n",
    "print(base_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- Untrained model performance is really bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UVe2kiB510Z"
   },
   "source": [
    "### **Question 6 Fine-Tuning Bert Model on training set (2 Marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Shjp4bPVlfMh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_classes = y.nunique()\n",
    "# Model initialization using BERT for sequence classification\n",
    "ft_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "f1Psz_7a59TU"
   },
   "outputs": [],
   "source": [
    "# setting the learning rate for the optimizer\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Setting the optimizer to Adam\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "\n",
    "# Specify the loss function for the model\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Define evaluation metric(s) for the model\n",
    "metric = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
    "\n",
    "# Compile the model with the chosen optimizer, loss function, and metrics\n",
    "ft_model.compile(optimizer=optimizer, loss=loss, metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ZJiQN-mb6kIn"
   },
   "outputs": [],
   "source": [
    "# Calculate class weights for imbalanced dataset\n",
    "cw = (y_train_enc.shape[0]) / np.bincount(y_train_enc)\n",
    "\n",
    "# Create a dictionary mapping class indices to their respective class weights\n",
    "cw_dict = {}\n",
    "for i in range(cw.shape[0]):\n",
    "    cw_dict[encoder.transform(encoder.classes_)[i]] = cw[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "YzNLc2n96oT_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722348003.388648   35149 service.cc:146] XLA service 0x7b38d4b74ac0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1722348003.388680   35149 service.cc:154]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2024-07-30 14:00:03.395264: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-30 14:00:03.415715: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "I0000 00:00:1722348003.504914   35149 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 73s 845ms/step - loss: 7.8403 - accuracy: 0.5275\n"
     ]
    }
   ],
   "source": [
    "# Number of training epochs\n",
    "n_epochs = 1\n",
    "#train bert model\n",
    "bert_base_tuned = ft_model.fit(train_tokenized_tf, epochs=n_epochs, class_weight=cw_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQ43M02s68Uf"
   },
   "source": [
    "### **Question 7. Evaluating the trained model performance (1 Mark)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "wd890cvZ61_o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 6s 281ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate raw predictions on the test dataset using the trained model\n",
    "preds_raw_val_ft = ft_model.predict(test_tokenized_tf)\n",
    "\n",
    "# Extract predicted labels by finding the index with the highest probability for each example\n",
    "preds_val_ft = np.argmax(np.array(tf.nn.softmax(preds_raw_val_ft.logits)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 3, 1, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_val_ft[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "D33p2Wqn7Tgq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "# Evaluate bert trained model\n",
    "ft_f1_score = bert_f1_score(y_test_enc, preds_val_ft)\n",
    "print(ft_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iBOLR6a7dbZ"
   },
   "source": [
    "### **Question 8: Write your observations (1 Mark)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training improved f1 score dramatically, from 0.09 to 0.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_wRqgV3B_8h"
   },
   "source": [
    "# **Prompt Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSiuLz5JCLQI"
   },
   "source": [
    "## **Section 2: Install Libraries for Prompt Engineering and Setting up Mistral Model (3 Marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcGMygn3_Mq9"
   },
   "source": [
    "### **Question 9: Install neccessary libraries (1 Mark)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gl0E64ce8Uct"
   },
   "outputs": [],
   "source": [
    "# Installation for GPU llama_cpp_python==0.2.28\n",
    " \"__________\"\n",
    "# For downloading the models from HF Hub huggingface-hub==0.23.2\n",
    " \"__________\"\n",
    "# install evaluate==0.4.2 and bert-score==0.3.13 using pip command\n",
    " \"__________\"\n",
    "# install numpy==1.25.2\n",
    " \"__________\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TsrUPUGTANTi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 14:03:44.509543: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-30 14:03:44.526300: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 14:03:44.546795: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 14:03:44.553085: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 14:03:44.568380: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 14:03:45.545633: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlocale\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Basic Imports for Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "from google.colab import drive\n",
    "import locale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdQdNav7AOtV"
   },
   "source": [
    "### **Question 10: Importing Libaries and Setting up Mistral Model (2 Marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO7TfAEiSxMR"
   },
   "source": [
    "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/blob/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cpQlIfeqAkvz"
   },
   "outputs": [],
   "source": [
    "## Import Hf_hub_download from hugging_face_hub\n",
    "# using pre-downloaded model\n",
    "\n",
    "## Import Llama from llama_cpp\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e9c3ISlwftKB"
   },
   "outputs": [],
   "source": [
    "# Define the model name or path as a string (You can find this info from hugging face website) Use Mistral\n",
    "\n",
    "model_name_or_path = \"/home/ubuntu/models/\"\n",
    "\n",
    "# Define the model basename as a string, indicating it's in the gguf format\n",
    "\n",
    "model_basename = \"mistral-7b-instruct-v0.2.Q5_K_M.gguf\" # the model is in gguf format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vvPmPoy4wCIc"
   },
   "outputs": [],
   "source": [
    "model_path = model_name_or_path+model_basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "R7Sm6O-jfzRo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /home/ubuntu/models/mistral-7b-instruct-v0.2.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    85.94 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4807.05 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the 'Llama' class with specified parameters\n",
    "# remove the blank spaces and complete the code\n",
    "\n",
    "lcpp_llm = Llama(\n",
    "        model_path=model_path,\n",
    "        n_threads=-1,  # CPU cores\n",
    "        n_batch=512,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "        n_gpu_layers=-1,  # Change this value based on your model and your GPU VRAM pool.\n",
    "        n_ctx=4096,  # Context window\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cj8Gk-m1Iz1q"
   },
   "source": [
    "## **Section 3: Text to Label (12 Marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6xcR3o2ga6p"
   },
   "source": [
    "# **Zero-Shot Prompting (6 Marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ8ual1jghmN"
   },
   "source": [
    "### **Q11: Define the Prompt Template, System Message, generate_prompt** **(3 Marks)**\n",
    "\n",
    "Define a **system message** as a string and assign it to the variable system_message to generate product class.\n",
    "\n",
    "Create a **zero shot prompt template** that incorporates the system message and user input.\n",
    "\n",
    "Define **generate_prompt** function that takes both the system_message and user_input as arguments and formats them into a prompt template\n",
    "\n",
    "\n",
    "Write a Python function called **generate_mistral_response** that takes a single parameter, narrative, which represents the user's complain. Inside the function, you should perform the following tasks:\n",
    "\n",
    "\n",
    "- **Combine the system_message and narrative to create a prompt string using generate_prompt function.**\n",
    "\n",
    "*Generate a response from the Mistral model using the lcpp_llm instance with the following parameters:*\n",
    "\n",
    "- prompt should be the combined prompt string.\n",
    "- max_tokens should be set to 1200.\n",
    "- temperature should be set to 0.\n",
    "- top_p should be set to 0.95.\n",
    "- repeat_penalty should be set to 1.2.\n",
    "- top_k should be set to 50.\n",
    "- stop should be set as a list containing '/s'.\n",
    "- echo should be set to False.\n",
    "Extract and return the response text from the generated response.\n",
    "\n",
    "Don't forget to provide a value for the system_message variable before using it in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QIMDLp51ghGr"
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are an AI evaluating input text from which to generate a product classification.\n",
    "Be concise.\n",
    "If you cannot determine a classification to at least 80% probability, respond with 'I cannot classify this.'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "e24qaY7ug2CC"
   },
   "outputs": [],
   "source": [
    "zero_shot_prompt_template = \"{input}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JKOyehERgy77"
   },
   "outputs": [],
   "source": [
    "# Define function that combines user_prompt and system_message to create the prompt\n",
    "def generate_prompt(_system_message, _user_input):\n",
    "    _prompt = f\"[INST] <<SYS>> {_system_message} <<SYS>> {_user_input} [/INST]\"\n",
    "    return _prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[INST] <<SYS>> You are an AI evaluating input text from which to generate a product classification.\\nBe concise.\\nIf you cannot determine a classification to at least 80% probability, respond with 'I cannot classify this.'\\n <<SYS>> purchase order day shipping amount receive product week sent followup email exact verbiage paid two day shipping received order company responded im sorry inform due unusually high order volume order shipped several week stock since early due high demand although continuing take order guaranteeing receive order place due time mask order exact shipping date right however guarantee ship soon soon delivers product u getting small shipment shipping first come first served basis appreciate patience fulfill order quickly recommend keeping order lose place line cancel distributor stock moment prefer cancel please note ask via email cancel accordance cancellation policy agreed checkout electronic inventory online requested order canceled refund issued canceled order sent verification order canceled refunded item particulate respirator refunded subtotal shipping tax total usd visa ending refund called disputed amount stated nothing needed submitted address issue recharged item removing called back dispute amount transaction rebillmerchandiserobert ca purchased thu posted wed purchased appears statement transaction rebill ca u followed see status case submitted documentation showing canceled order supposed submit refund called back speak manager case stated dispute ruled favor charge removed card capital one removed purchase bill purchase adjustmentmerchandiserobert j posted fri purchased appears statement purchase adjustment capital one recharges amount transaction rebillmerchandiserobert j purchased thu posted mon purchased appears statement transaction rebill called capital one requested recharge stated visa ruled case pretended remove purchase knew anything case manager ruling favor [/INST]\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_prompt(system_message, zero_shot_prompt_template.format(input=data.iloc[0]['narrative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "3PJcu3dwg7FO"
   },
   "outputs": [],
   "source": [
    "def generate_mistral_response(input_text):\n",
    "\n",
    "    # Combine user_prompt and system_message to create the prompt\n",
    "    prompt=generate_prompt(system_message, input_text)\n",
    "    # Generate a response from the LLaMA model\n",
    "    response = lcpp_llm(\n",
    "        prompt=prompt,\n",
    "        max_tokens=1200,\n",
    "        temperature=0,\n",
    "        top_p=0.95,\n",
    "        repeat_penalty=1.2,\n",
    "        top_k=50,\n",
    "        stop=['/s'],\n",
    "        echo=False\n",
    "    )\n",
    "\n",
    "    # Extract and return the response text\n",
    "    response_text = response[\"choices\"][0][\"text\"]\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip-install-2shwm50v/llama-cpp-python_54e6ad3a21354de2a272cd641f6dc4df/vendor/llama.cpp/src/llama.cpp:14550: GGML_ASSERT(n_threads > 0) failed\n"
     ]
    }
   ],
   "source": [
    "generate_mistral_response(zero_shot_prompt_template.format(input=data.iloc[0]['narrative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmFU17EpyeKg"
   },
   "source": [
    "**Due to limited GPU resources, we will test our model with zero prompts on only 50 examples instead of the entire dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhwItJrIhNta"
   },
   "outputs": [],
   "source": [
    "# Randomly select 50 rows\n",
    "new_data = data.sample(n=50, random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-GrdMrVkaei"
   },
   "source": [
    "### **Q12: Create a new column in the DataFrame called 'mistral_response' and populate it with responses generated by applying the 'generate_mistral_response' function to each 'narrative' in the DataFrame and prepare the mistral_response_cleaned column using extract_category function** **(1 Marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqEme1elh24Y"
   },
   "outputs": [],
   "source": [
    "# example - new_data['mistral_response'] = new_data['narrative'].apply(lambda x:______ )\n",
    "new_data['mistral_response'] = \"______ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1CBbpD3m_rf"
   },
   "outputs": [],
   "source": [
    "new_data['mistral_response'] = \"______ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10Bulypzkwjt"
   },
   "outputs": [],
   "source": [
    "def extract_category(text):\n",
    "    # Define the regex pattern to match \"category:\" or \"Category:\" followed by a word\n",
    "    pattern = r'category:\\s*(\\w+)'  # The pattern itself remains the same\n",
    "\n",
    "    # Use re.search with the re.IGNORECASE flag to make it case-insensitive\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "\n",
    "    # If a match is found, return the captured group, else return None\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        pattern1 = r'(credit_card|retail_banking|credit_reporting|mortgages_and_loans|debt_collection)'\n",
    "        match = re.search(pattern1, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group()\n",
    "        else:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-REbjsCnhaq"
   },
   "outputs": [],
   "source": [
    "# example - new_data['mistral_response_cleaned'] = new_data['narrative'].apply(lambda x:______ )\n",
    "new_data['mistral_response_cleaned'] = \"______ \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9t6gsZVsL2b"
   },
   "source": [
    "### **Q14: Calculate the F1 score** **(1 Marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CeUoZb4ttDCL"
   },
   "outputs": [],
   "source": [
    "# Calculate F1 score for 'product' and 'mistral_response'\n",
    "f1 =  \"______ \"\n",
    "\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynzQvTi8tCcp"
   },
   "outputs": [],
   "source": [
    "# Calculate F1 score for 'product' and 'mistral_response_cleaned'\n",
    "f2 =  \"______ \"\n",
    "print(f'F1 Score: {f2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "803ZuG7OtSSj"
   },
   "source": [
    "### **Q15: Explain the difference in F1 scores between mistral_response and mistral_response_cleaned.** **(1 Marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0YUCSXAnx3y"
   },
   "source": [
    "# **Few-Shot Prompting (6 Marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgxdRjW_wHId"
   },
   "source": [
    "### **Q16: Prepare examples for a few-shot prompt, formulate the prompt, and generate the Mistral response. (4 Marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPuH7_ravBtS"
   },
   "source": [
    "**Generate a set of gold examples by randomly selecting 10 instances of user_input and assistant_output from dataset ensuring a balanced representation with 2 examples from each class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iO9Wj19_n_LO"
   },
   "outputs": [],
   "source": [
    "# Separate positive and negative reviews\n",
    "import json\n",
    "review_1 = data[data['product'] == 'credit_card']\n",
    "review_2 = data[data['product'] == 'retail_banking']\n",
    "review_3 = data[data['product'] == 'credit_reporting']\n",
    "review_4 = data[data['product'] == 'mortgages_and_loans']\n",
    "review_5 = data[data['product'] == 'debt_collection']\n",
    "\n",
    "# Sample 3 positive and 3 negative reviews for gold examples\n",
    "gold_examples_1 = review_1.sample(2, random_state=40)\n",
    "gold_examples_2 = review_2.sample(2, random_state=40)\n",
    "gold_examples_3 = review_3.sample(2, random_state=40)\n",
    "gold_examples_4 = review_4.sample(2, random_state=40)\n",
    "gold_examples_5 = review_5.sample(2, random_state=40)\n",
    "\n",
    "# Concatenate positive and negative gold examples\n",
    "gold_examples_df = pd.concat([gold_examples_1,gold_examples_2,gold_examples_3,gold_examples_4,gold_examples_5 ])\n",
    "\n",
    "# Create the training set by excluding gold examples\n",
    "test_df = data.drop(index=gold_examples_df.index)\n",
    "\n",
    "# Convert gold examples to JSON\n",
    "columns_to_select = ['narrative', 'product']\n",
    "gold_examples_json = gold_examples_df[columns_to_select].to_json(orient='records')\n",
    "\n",
    "# Print the first record from the JSON\n",
    "print(json.loads(gold_examples_json)[0])\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(\"Test Set Shape:\", examples_df.shape)\n",
    "print(\"Gold Examples Shape:\", gold_examples_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1i8taCkTwqOl"
   },
   "source": [
    "Define your **system_message**.\n",
    "\n",
    "Define **first_turn_template**, **example_template** and **prediction template**\n",
    "\n",
    "**create few shot prompt** using gold examples and system_message\n",
    "\n",
    "Randomly select 50 rows from test_df as test_data\n",
    "\n",
    "Create **mistral_response** and **mistral_response_cleaned** columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPzpBeWmzSIH"
   },
   "outputs": [],
   "source": [
    "system_message = \"______ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bPjQJs_zRoo"
   },
   "outputs": [],
   "source": [
    "first_turn_template = \"______ \"\n",
    "examples_template = \"______ \"\n",
    "prediction_template = \"______ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X07_kNji0vTp"
   },
   "outputs": [],
   "source": [
    "def create_few_shot_prompt(system_message, examples):\n",
    "\n",
    "    \"\"\"\n",
    "    Return a prompt message in the format expected by Mistral 7b.\n",
    "    10 examples are selected randomly as golden examples to form the\n",
    "    few-shot prompt.\n",
    "    We then loop through each example and parse the narrative as the user message\n",
    "    and the product as the assistant message.\n",
    "\n",
    "    Args:\n",
    "        system_message (str): system message with instructions for classification\n",
    "        examples(DataFrame): A DataFrame with examples (product + narrative + summary)\n",
    "        to form the few-shot prompt.\n",
    "\n",
    "    Output:\n",
    "        few_shot_prompt (str): A prompt string in the Mistral format\n",
    "    \"\"\"\n",
    "\n",
    "    few_shot_prompt = ''\n",
    "\n",
    "    columns_to_select = \"__________\"\n",
    "\n",
    "    examples = (\n",
    "        examples_df.loc[:, columns_to_select].to_json(orient='records')\n",
    "    )\n",
    "\n",
    "    for idx, example in enumerate(json.loads(examples)):\n",
    "        user_input_example = \"__________\"\n",
    "        assistant_output_example = \"__________\"\n",
    "\n",
    "        if idx == 0:\n",
    "            few_shot_prompt += mistral_first_turn_template.format(\n",
    "                system_message=system_message,\n",
    "                user_message=user_input_example,\n",
    "                assistant_message=assistant_output_example\n",
    "            )\n",
    "        else:\n",
    "            few_shot_prompt += mistral_examples_template.format(\n",
    "                user_message=user_input_example,\n",
    "                assistant_message=assistant_output_example\n",
    "            )\n",
    "\n",
    "    return few_shot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHfPlD9o1G7M"
   },
   "outputs": [],
   "source": [
    "few_shot_prompt = \"______ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmTmY2oq3PRv"
   },
   "outputs": [],
   "source": [
    "def generate_prompt(few_shot_prompt,new_review):\n",
    "    prompt =  \"______ \"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKrndCFRCJhK"
   },
   "outputs": [],
   "source": [
    "def generate_mistral_response(input_text):\n",
    "\n",
    "    # Combine user_prompt and system_message to create the prompt\n",
    "    prompt = \"__________\"\n",
    "\n",
    "    # Generate a response from the LLaMA model\n",
    "    response = lcpp_llm(\n",
    "    )\n",
    "\n",
    "    # Extract and return the response text\n",
    "    response_text = response[\"choices\"][0][______]  ### Fill in the blank\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKYOjm7LDLJj"
   },
   "outputs": [],
   "source": [
    "# Randomly select 50 rows\n",
    "test_data = data.sample(n=50, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdsjoaP4DXIk"
   },
   "outputs": [],
   "source": [
    "test_data['mistral_response'] = \"______ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rA9KYTgxDh2F"
   },
   "outputs": [],
   "source": [
    "test_data['mistral_response_cleaned'] = \"______ \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plQNsfl0EDSG"
   },
   "source": [
    "### **Calculate F1 score (1 Mark)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WXEgmPKEIoi"
   },
   "outputs": [],
   "source": [
    "# Calculate F1 score for 'product' and 'mistral_response_cleaned'\n",
    "f3 =  \"______ \"\n",
    "print(f'F1 Score: {f3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdtvdEnTEZcG"
   },
   "source": [
    "### **Q17: Share your observations on the few-shot and zero-shot prompt techniques. (1 Marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vca9B37WGneH"
   },
   "source": [
    "## **Section 4: Text to Text generation (5 Marks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0FEJZSDL2U1"
   },
   "source": [
    "Define a **system message** as a string and assign it to the variable system_message to generate product class.** (1 Mark)**\n",
    "\n",
    "Create a **zero shot prompt template** that incorporates the system message and user input.\n",
    "\n",
    "Define **generate_prompt** function that takes both the system_message and user_input as arguments and formats them into a prompt template\n",
    "\n",
    "\n",
    "Write a Python function called **generate_mistral_response** that takes a single parameter, narrative, which represents the user's complain. Inside the function, you should perform the following tasks:\n",
    "\n",
    "\n",
    "- **Combine the system_message and narrative to create a prompt string using generate_prompt function.**\n",
    "\n",
    "*Generate a response from the Mistral model using the lcpp_llm instance with the following parameters:*\n",
    "\n",
    "- prompt should be the combined prompt string.\n",
    "- max_tokens should be set to 1200.\n",
    "- temperature should be set to 0.\n",
    "- top_p should be set to 0.95.\n",
    "- repeat_penalty should be set to 1.2.\n",
    "- top_k should be set to 50.\n",
    "- stop should be set as a list containing '/s'.\n",
    "- echo should be set to False.\n",
    "Extract and return the response text from the generated response.\n",
    "\n",
    "Don't forget to provide a value for the system_message variable before using it in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PewmIuBII56"
   },
   "outputs": [],
   "source": [
    "system_message = \"__________\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czkz3BMpL2ek"
   },
   "outputs": [],
   "source": [
    "zero_shot_prompt_template = \"__________\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlY1x6ACLtlL"
   },
   "outputs": [],
   "source": [
    "# Define function that combines user_prompt and system_message to create the prompt\n",
    "def generate_prompt(system_message,user_input):\n",
    "    prompt = \"__________\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-kqoM4YL1xU"
   },
   "outputs": [],
   "source": [
    "def generate_mistral_response(input_text):\n",
    "\n",
    "    # Combine user_prompt and system_message to create the prompt\n",
    "    prompt = \"__________\"\n",
    "\n",
    "    # Generate a response from the LLaMA model\n",
    "    response = lcpp_llm(\n",
    "    )\n",
    "\n",
    "    # Extract and return the response text\n",
    "    response_text = response[\"choices\"][0][______]  ### Fill in the blank\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpiIHpThMtW6"
   },
   "source": [
    "### **Q19: Generate mistral_response column containing LLM generated summaries** **(1 Marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fqxl54MMMBTu"
   },
   "outputs": [],
   "source": [
    "# Randomly select 50 rows\n",
    "test_data = data.sample(n=50, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kWS3MmUMEgc"
   },
   "outputs": [],
   "source": [
    "test_data['mistral_response'] = \"______ \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dl-5fRKbND2n"
   },
   "source": [
    "### **Q20: Evaluate bert score** **(2 Marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6SqvLIeMVN-"
   },
   "outputs": [],
   "source": [
    "def evaluate_score(result, scorer, bert_score=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Return the ROUGE score or BERTScore for predictions on gold examples\n",
    "    For each example we make a prediction using the prompt.\n",
    "    Gold summaries and the AI generated summaries are aggregated into lists.\n",
    "    These lists are used by the corresponding scorers to compute metrics.\n",
    "    Since BERTScore is computed for each candidate-reference pair, we take the\n",
    "    average F1 score across the gold examples.\n",
    "\n",
    "    Args:\n",
    "        prompt (List): list of messages in the Open AI prompt format\n",
    "        gold_examples (str): JSON string with list of gold examples\n",
    "        scorer (function): Scorer function used to compute the ROUGE score or the\n",
    "                           BERTScore\n",
    "        bert_score (boolean): A flag variable that indicates if BERTScore should\n",
    "                              be used as the metric.\n",
    "\n",
    "    Output:\n",
    "        score (float): BERTScore or ROUGE score computed by comparing model predictions\n",
    "                       with ground truth\n",
    "    \"\"\"\n",
    "\n",
    "    model_predictions = result['mistral_response'].tolist\n",
    "    ground_truths = result['summary'].tolist()\n",
    "    if bert_score:\n",
    "        score = scorer.compute(\n",
    "            predictions=model_predictions,\n",
    "            references=ground_truths,\n",
    "            lang=\"en\",\n",
    "            rescale_with_baseline=True\n",
    "        )\n",
    "\n",
    "        return sum(score['f1'])/len(score['f1'])\n",
    "    else:\n",
    "        return scorer.compute(\n",
    "            predictions=model_predictions,\n",
    "            references=ground_truths\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4T4k6rA_MlS-"
   },
   "outputs": [],
   "source": [
    "bert_scorer = \"__________\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikncTGPBNPRb"
   },
   "outputs": [],
   "source": [
    "score = \"__________\"\n",
    "print(f'BERTScore: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-p6_jPcVNWz7"
   },
   "source": [
    "### **Q21: Write your observation** **(1 Marks)**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
